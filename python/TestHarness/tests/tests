[Tests]
    design = 'TestHarness.md'
    issues = '#427'
    parallel_scheduling = True

  [long_running]
    type = PythonUnitTest
    input = test_LongRunning.py
    requirement = "The system shall report a non-failing status after a predetermined time of no activity"
    issues = '#9280'
  []
  [longest_jobs]
    type = PythonUnitTest
    input = test_LongestJobs.py
    requirement = "The system shall support the output of the longest running jobs"
    issues = '#16752'
  []
  [csvdiffs]
    type = PythonUnitTest
    input = test_CSVDiffs.py
    requirement = "The system shall report a failure when encountering a CSV differential result"
    issues = '#11250 #11251'
    collections = 'FAILURE_ANALYSIS'
  []
  [diff]
    type = PythonUnitTest
    input = test_Diff.py
    requirement = "The system shall report a failure when encountering differential result"
    issues = '#8373'
    collections = 'FAILURE_ANALYSIS'
  []
  [diff_gold]
    type = PythonUnitTest
    input = test_DiffGold.py
    requirement = "The system shall report a failure when encountering differential result with a custom gold directory."
    issues = '#10647'
    collections = 'FAILURE_ANALYSIS'
  []
  [format_result]
    type = PythonUnitTest
    input = test_FormatResult.py
    requirement = "The system shall support the output of single-lined test results with a user configurable format string"
    issues = '#30755'
  []
  [min_ad_size]
    type = PythonUnitTest
    input = test_MinADSize.py
    requirement = "The system shall restrict tests based on the available dual number derivative vector size"
  []
  [cyclic]
    type = PythonUnitTest
    input = test_Cyclic.py
    requirement = "The system shall report a failure during a cyclic dependency event"
    collections = 'FAILURE_ANALYSIS'
  []
  [dependency_skip]
    type = PythonUnitTest
    input = test_DependencySkip.py
    requirement = "The system shall not perform a test if said test has a skipped dependency"
  []
  [missing_gold]
    type = PythonUnitTest
    input = test_MissingGold.py
    requirement = "The system shall report a failure if a test is missing its gold file"
    collections = 'FAILURE_ANALYSIS'
  []
  [expect]
    type = PythonUnitTest
    input = test_Expect.py
    requirement = "The system shall report a failure if expected output is not reported"
    issues = "#9933"
    collections = 'FAILURE_ANALYSIS'
  []
  [duplicate]
    type = PythonUnitTest
    input = test_Duplicate.py
    prereq = should_execute
    requirement = "The system shall report a failure if test output causes a race condition"
    collections = 'FAILURE_ANALYSIS'
  []
  [deleted]
    type = PythonUnitTest
    input = test_Deleted.py
    requirement = "The system shall report deleted tests as failures when specified with additional --extra-info options"
                  "In all other cases, deleted tests will be treated as skipped tests"
  []
  [dry_run]
    type = PythonUnitTest
    input = test_DryRun.py
    requirement = "The system shall perform all operations required of the TestHarness except executing a test"
    issues = "#8637"
  []
  [display_required]
    type = PythonUnitTest
    input = test_DisplayRequired.py
    requirement = "The system shall run only tests designated with display_required."
    issues = '#8700 #8701'
  []
  [ignore]
    type = PythonUnitTest
    input = test_Ignore.py
    requirement = "The system shall allow users to ignore and override specified prerequisites"
  []
  [timeout]
    type = PythonUnitTest
    input = test_Timeout.py
    requirement = "The system shall report a failure if a test exceeds a predetermined walltime"
    collections = 'FAILURE_ANALYSIS'
  []
  [unknown_prereq]
    type = PythonUnitTest
    input = test_UnknownPrereq.py
    requirement = "The system shall report a failure if a test depends on another non-existent test"
    collections = 'FAILURE_ANALYSIS'
  []
  [syntax]
    type = PythonUnitTest
    input = test_Syntax.py
    requirement = "The system shall report a failure due to issues with input files"
    issues = "#9249"
    collections = 'FAILURE_ANALYSIS'
  []
  [required_apps]
    type = PythonUnitTest
    input = test_RequiredApps.py
    requirement = "The system shall skip a test if required application is unavailable"
    issues = '#11095'
  []
  [should_execute]
    type = PythonUnitTest
    input = test_ShouldExecute.py
    requirement = "The system shall only perform the validation of test results without executing the test itself"
    issues = '#9932'
  []
  [report_skipped]
    type = PythonUnitTest
    input = test_ReportSkipped.py
    requirement = "The system shall skip syntax only tests if instructed to do so"
    issues = '#9359'
  []
  [distributed_mesh]
    type = PythonUnitTest
    input = test_DistributedMesh.py
    requirement = "The system shall properly run tests using distributed mesh options"
    issues = '#9181'
  []
  [allocations]
    type = PythonUnitTest
    input = test_Allocations.py
    requirement = "The system shall supply the necessary resources a test requires, and report when these resources are insufficient to run said test"
    issues = '#10272'
  []
  [extra_info]
    type = PythonUnitTest
    input = test_ExtraInfo.py
    requirement = "The system shall print all caveats pertaining to the test involved"
    issues = '#10272'
  []
  [parser_errors]
    type = PythonUnitTest
    input = test_ParserErrors.py
    requirement = "The system shall report a failure if a test file is not constructed properly or does not contain valid parameters"
    issues = '#10400'
    collections = 'FAILURE_ANALYSIS'
  []
  [arbitrary_tests]
    type = PythonUnitTest
    input = test_ArbitrarySpecFile.py
    requirement = "The system shall perform normal operating procedures on a single provided test spec file"
    issues = '#11076'
  []
  [write_results]
    type = PythonUnitTest
    input = test_WriteResults.py
    prereq = 'diff'
    requirement = "The system shall write the output (stdout|stderr) that an executed test generated to a file as designated by user supplied arguments"
    issues = '#11116'
  []
  [recover_tests]
    type = PythonUnitTest
    input = test_Recover.py
    requirement = "The system shall be able to perform recovery of a test"
    issues = '#11492'
  []
  [trim_output]
    type = PythonUnitTest
    input = test_TrimOutput.py
    requirement = "The system shall trim output once threshold has exceeded"
    issues = '#12167'
  []
  [race_conditions]
    type = PythonUnitTest
    input = test_RaceConditions.py
    requirement = "The system shall detect and report race conditions that exist in the supplied tests"
    issues = '#13186'
  []
  [unreadable_output]
    type = PythonUnitTest
    input = test_UnreadableOutput.py
    requirement = "The system shall detect and report unreadable output in executed commands"
    issues = '#14370'
  []
  [failed_tests]
    type = PythonUnitTest
    input = test_FailedTests.py
    requirement = "The system shall run only tests which previously have failed"
    issues = '#14512'
  []
  [python_version]
    type = PythonUnitTest
    input = test_PythonVersion.py
    requirement = "The system shall support restrictions based on the python version available."
    issues = '#13903'
  []
  [csvvalidationtester]
    type = PythonUnitTest
    input = test_CSVValidationTester.py
    requirement = "The system shall be able to compare computed values against measured data using mean value and standard deviation"
    issues = '#14511'
  []
  [working_directory]
    type = PythonUnitTest
    input = test_WorkingDirectory.py
    requirement = "The system shall be able to run tests in relative path directories supplied by the spec file"
    issues = '#14962'
  []
  [unknown_param]
    type = PythonUnitTest
    input = test_UnknownParam.py
    requirement = "The system shall produce a descriptive error with the file and line number when a test specification parameter is unknown."
    issues = '#14803'
    collections = 'FAILURE_ANALYSIS'
  []
  [do_last]
    type = PythonUnitTest
    input = test_DoLast.py
    requirement = "The system shall perform a test after all other tests have passed if specified to do so"
    issues = '#15230'
  []
  [schema_expect_err]
    type = PythonUnitTest
    input = test_SchemaDiff.py
    requirement = "The system shall report multiple failures resulting from SchemaDiff operations."
    required_python_packages = 'deepdiff xmltodict'
    collections = 'FAILURE_ANALYSIS'
  []
  [test_show_last_results]
    type = PythonUnitTest
    input = test_Replay.py
    requirement = "The system shall be able to replay last results"
    issues = '#22545'
  []
  [custom_eval]
    type = PythonUnitTest
    input = test_CustomEval.py
    issues = '#22946'
    requirement = "The system shall be able to evaluate a given test with a user-supplied evaluation function."
  []
  [test_install_type]
    type = PythonUnitTest
    input = test_InstallType.py
    requirement = "The system shall skip tests not capable of being run depending on binary installation type"
    issues = '#24195'
  []
  [test_machine_type]
    type = PythonUnitTest
    input = test_MachineType.py
    requirement = "The system shall skip tests not capable of being run depending on micro architecture"
    issues = '#25317'
  []
  [test_soft_heavy]
    type = PythonUnitTest
    input = test_SoftHeavyDependency.py
    requirement = "The system shall not skip non-heavy tests for which heavy tests depend on"
    issues = '#26215'
  []
  [test_output_interface]
    type = PythonUnitTest
    input = test_OutputInterface.py
    requirement = 'The system shall provide a common interface for storing and retrieving output that supports sanitization
    issues = '#27562'
  []
  [test_method]
    type = PythonUnitTest
    input = test_GetExecutable.py
    requirement = "The system shall test the functionality of the TestHarness.getExecutable method."
    issues = '#30200'
  []
  [test_capture_perf_graph]
    type = PythonUnitTest
    input = test_CapturePerfGraph.py
    requirement = "The system shall support systematically capturing performance statistics from the execution of MOOSE application tests."
    issues = '#30569'
  []
  [validationcase]
    type = PythonUnitTest
    input = test_ValidationCase.py
    requirement = 'The system shall provide an interface for storing results and data for validation cases'
    issues = '#30517'
  []
  [validation]
    type = PythonUnitTest
    input = test_Validation.py
    requirement = 'The system shall support storing results and data for validation cases that are combined with a standard test'
    issues = '#30517'
  []
  [resultsreader_results]
    type = PythonUnitTest
    input = test_resultsreader_results.py
    requirement = 'The system shall provide a python interface for interacting with previous test results stored in a database'
    issues = '#30819'
  []
  [resultsreader_reader]
    type = PythonUnitTest
    input = test_resultsreader_reader.py
    requirement = 'The system shall provide a python interface for loading previous test results from a database'
    issues = '#30819'
  []
  [augmented_capabilities]
    type = PythonUnitTest
    input = test_AugmentedCapabilities.py
    requirement = 'The system shall provide the ability to set whether or not a test can run using the capabilities system using properties of the currently running test suite.'
    issues = '#31135'
  []
  [require_capability]
    type = PythonUnitTest
    input = test_RequireCapability.py
    requirement = 'The system shall provide the ability to filter tests based on whether or not they are dependent on certain capabilities.'
    issues = '#31310'
  []
[]
