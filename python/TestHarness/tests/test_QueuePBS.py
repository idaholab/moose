#!/usr/bin/env python
#* This file is part of the MOOSE framework
#* https://www.mooseframework.org
#*
#* All rights reserved, see COPYRIGHT for full restrictions
#* https://github.com/idaholab/moose/blob/master/COPYRIGHT
#*
#* Licensed under LGPL 2.1, please see LICENSE for details
#* https://www.gnu.org/licenses/lgpl-2.1.html

from contextlib import contextmanager
from TestHarness import TestHarness
from TestHarness import util
import mock
import os, sys, unittest, time, shutil, json


class HarnessAndDAG(object):
    """
    A context manager for properly handling the scheduler thread pools using
    with statements.
    """
    def __init__(self, pbs_file):
        self.pbs_session_file = pbs_file
        self.harness = None

    def createHarnessAndDAG(self, test_file):
        """
        Return a tuple of the TestHarness, and the job_dag containing
        the testers generated by parsing 'test_file'.

        In order to do this, we have to schedule the actual test, but
        prevent (mock) the scheduler's ability to assign any threads to
        do work.

        By doing this, its important to know we need to properly close
        the threading pools when we are done;  harness.waitFinish().

        Calling harness.waitFinish() is also the only way the
        QueueManager knows when its safe to write the session file.
        """

        ### We need to simulate what run_tests would do. Meaning, we need to
        ### instance the TestHarness while being in the directory where run_test
        ### sits. This is needed so tester's TestName is properly set (TestHarness
        ### takes the path leading up to run_tests, strips this off, and the
        ### results is what makes up the actual TestName).
        saved_cwd = os.getcwd()
        run_tests_dir = os.path.join(os.getenv('MOOSE_DIR'), 'test')
        os.chdir(run_tests_dir)

        # Instance the TestHarness
        self.harness = TestHarness(['foo', '--pbs', self.pbs_session_file], os.getenv('MOOSE_DIR'), app_name='moose_test')

        ### With the TestHarness properly instanced we now need to simulate
        ### what findAndRunTests does by changing into each tester directory
        ### before asking the factory to parse the file.
        tester_dir = os.path.dirname(test_file)
        tester_file = os.path.basename(test_file)
        sys.path.append(tester_dir)
        os.chdir(tester_dir)

        # Create the testers
        testers = self.harness.createTesters(tester_dir, tester_file, False)

        ### change back to original dir
        os.chdir(saved_cwd)
        sys.path.pop()

        ### TODO: that was a lot of chdir's. Lets figure out a way for the
        ### TestHarness to not have to do this. In actuality, we should only
        ### have had to do two things: Instance the TestHarness and create
        ### the testers.

        with mock.patch.object(self.harness.scheduler, 'queueJobs') as mock_queue_jobs:
            mock_queue_jobs.return_value = True
            self.harness.scheduler.schedule(testers)
            (args, kwargs) = mock_queue_jobs.call_args
            job_dag = kwargs['run_jobs'][0].getDAG()

        #### SANITY CHECK ####
        if job_dag.size() != 2:
            raise Exception('Sanity Check Failed: The test file has been modified but not the unittest')

        return (self.harness, job_dag)

    def close(self):
        """ shutdown the scheduler's thread pools """
        if self.harness:
            # Force zero job count, to handle the possibility a unittest failed
            # without the TestHarness knowing about the failure (unittest bombed
            # out while partially scheduling jobs). Basically, everywhere you see
            # self.fail('why'), is why this is needed.
            self.harness.scheduler.job_queue_count = 0
            self.harness.scheduler.waitFinish()

class TestHarnessTestCase(unittest.TestCase):
    def setUp(self):
        """
        setUp occurs before every test.

        Clean up previous session file and setup path locations.
        """
        self.pbs_session_file = os.path.join(os.getenv('MOOSE_DIR'), 'test', 'unittesting_pbs')
        self.test_file = os.path.join(os.getenv('MOOSE_DIR'), 'test','tests', 'test_harness', 'queue_job')
        self.session_dir = os.path.join(os.path.dirname(self.test_file), 'job_' + os.path.basename(self.pbs_session_file))

        with open('qstat_output.txt', 'r') as qstat_stdout_file:
            self.qstat_output = qstat_stdout_file.read()

        try:
            # remove previous session data and the session file
            self._clean()
            os.remove(self.pbs_session_file)
        except:
            pass

    def tearDown(self):
        """
        tearDown occurs after every test.

        Clean up old session file and sub-directories.
        """
        try:
            # remove previous session data and the session file
            self._clean()
            os.remove(self.pbs_session_file)
        except:
            pass

    def _clean(self):
        """ clean the session sub-directories only (leave session file intact) """
        if os.path.exists(self.session_dir):
            try:
                shutil.rmtree(self.session_dir)
            except:
                raise Exception('Failed to clean up pre-existing session directories')

    @mock.patch.object(util, 'formatResult')
    def _mockSinglePBSLaunch(self, harness, job, fake_result, mock_result):
        """
        Launch 'job' and mock the output of runCommand to 'fake_result', then
        return a list of the last thing(s) the TestHarness tried to print.

        Note: Because we are wanting to control each launched job, we must
        mock getNextJobGroup. But by doing so we need to become the Scheduler's
        job manager per say. Hence why, we are checking and modifying the
        tester's status.
        """

        # Limit printing of MagicMock
        mock_result.return_value = ''

        with mock.patch.object(harness.scheduler, 'getNextJobGroup') as mock_next_jobs:
            with mock.patch.object(util, 'runCommand') as mock_run:
                # Setup Mock returns
                mock_run.return_value = fake_result
                mock_next_jobs.return_value = []
                tester = job.getTester()

                # scheduler normally does this for us. But we are 'mocking'
                # a run, so we must adjust the status manually.
                if tester.isInitialized():
                    tester.setStatus('PENDING', tester.bucket_pending)

                # A different method may have adjusted this tester to be
                # skipped (jobA fails and thus skips and never runs jobB,
                # but _occures_ during jobA's iteration).
                elif tester.isSkipped():
                    return

                # Launch the job (non-blocking)
                harness.scheduler.queueJobs(run_jobs=[job])

                # Wait for test to finish
                while tester.isPending():
                    time.sleep(.1)

                # Build a list of results that may have occurred (skipped
                # dependencies are printed during the same job that failed,
                # thus creating two calls to formatResults results)
                result_list = mock_result.call_args_list
                result_output_list = []
                for single_result in result_list:
                    (result_output, tmp_kwargs) = single_result
                    # index 1 is the result variable in util.formatResult
                    result_output_list.append(result_output[1])

        return result_output_list

    def getQstatOutput(self, **kwargs):
        """ return valid qstat output """
        tmp_qstat = self.qstat_output
        for tag, value in kwargs.iteritems():
            tmp_qstat = tmp_qstat.replace('<' + str(tag).upper() + '>', str(value))

        # set any remaining unset tags to a working default:
        default_tags = [('<EXIT_STATUS>', '0'), ('<JOB_STATE>', 'F'), ('<JOB_ID>', '1')]
        for unset_tag in default_tags:
            tmp_qstat = tmp_qstat.replace(unset_tag[0], unset_tag[1])

        return tmp_qstat

    def yieldJobs(self, job_dag):
        """ return each job in the dag with an id """
        jobs = job_dag.topological_sort()
        for job_id, job in enumerate(jobs):
            yield (job_id, job)

    @contextmanager
    def harnessDAG(self):
        """
        Method to provide a means to create the TestHarness and properly
        close the thread pools when complete.
        """
        harness = HarnessAndDAG(self.pbs_session_file)
        harness_dag = harness.createHarnessAndDAG(self.test_file)
        try:
            yield harness_dag
        finally:
            harness.close()

    def testPBSGoodLaunch(self):
        """
        Test QueueManager's ability to launch jobs using qsub
        """

        # Initialize the TestHarness and retrieve our job_dag
        with self.harnessDAG() as harness_dag:
            (harness, job_dag) = harness_dag
            for job_id, job in self.yieldJobs(job_dag):
                result_list = self._mockSinglePBSLaunch(harness, job, '%d.nowhere.org' % (job_id))
                if len(result_list) == 1:
                    self.assertRegexpMatches(result_list[0], 'LAUNCHED %d' % (job_id))
                else:
                    self.fail('Failed: QueueManager launched one job, but received several results')

    def testPBSBadLaunch(self):
        """
        Test QueueManager's ability to handle a failed qsub launch
        """

        # Initialize the TestHarness and retrieve our job_dag
        with self.harnessDAG() as harness_dag:
            (harness, job_dag) = harness_dag
            for job_id, job in self.yieldJobs(job_dag):
                result_list = self._mockSinglePBSLaunch(harness, job, 'NO_ID.nowhere.org')

                # Join the results, because the TestHarness receives them out-of-order
                if job_id == 0:
                    join_results = ' '.join([result_list[0], result_list[1]])
                    self.assertRegexpMatches(join_results, 'FAILED \(QSUB FAILURE\)')
                    self.assertRegexpMatches(join_results, 'SKIP')

                # We _should_ actually have no results
                elif job_id == 1 and result_list != None:
                    self.fail('Failed: A job that should not have launched attempted to do so')

    def testPBSBadQstat(self):
        """
        Test QueueManager's ability to handle invalid qstat output
        """

        # Launch jobs correctly to create a session file (QUEUED status)
        self.testPBSGoodLaunch()

        with self.harnessDAG() as harness_dag:
            (harness, job_dag) = harness_dag

            ### Test for non-qstat type output (command not found or the like)
            for job_id, job in self.yieldJobs(job_dag):
                qstat_return = 'gibberish, rubbish, nothing good anyway.'
                result_list = self._mockSinglePBSLaunch(harness, job, qstat_return)

                # Join the results, because the TestHarness receives them out-of-order
                if job_id == 0:
                    join_results = ' '.join([result_list[0], result_list[1]])
                    self.assertRegexpMatches(join_results, 'FAILED \(INVALID QSTAT RESULTS\)')
                    self.assertRegexpMatches(join_results, 'SKIP')

                # We _should_ actually have no results
                elif job_id == 1 and result_list != None:
                    self.fail('Failed: A job that should not have launched attempted to do so')

    def testExceededWalltime(self):
        """
        Test QueueManagers ability to handle jobs killed by PBS due to
        exceeding walltime (exit code 271).

        Caveat (more of a note to myself, this stumped me...):
        Because we are mocking the results, we will never see the actual
        error we are testing for. Remember: @mockSinglePBS can not grab
        returned strings _from_ called methods (formatResults() in our
        case). Mock instead allows us to ask what _variables_ were
        _sent_ to them (using call_args).

        So, instead, we will interface with the tester directly and parse
        through the caveats searching for what we expect (getCaveats()).

        See 'testPBSProcessResults' below for 'NO STDOUT FILE' explination.
        """

        # Launch jobs correctly to create a session file (QUEUED status)
        self.testPBSGoodLaunch()

        with self.harnessDAG() as harness_dag:
            (harness, job_dag) = harness_dag

            ### Test for non-qstat type output (command not found or the like)
            for job_id, job in self.yieldJobs(job_dag):
                qstat_return = self.getQstatOutput(job_id=job_id, job_state='F', exit_status='271')
                result_list = self._mockSinglePBSLaunch(harness, job, qstat_return)
                tester = job.getTester()

                # Join the results, because the TestHarness receives them out-of-order
                if job_id == 0:
                    join_results = ' '.join([result_list[0], result_list[1]])
                    self.assertRegexpMatches(join_results, 'FAILED \(NO STDOUT FILE\)')
                    self.assertRegexpMatches(join_results, 'SKIP')

                    # check caveats for specific string
                    self.assertRegexpMatches(' '.join(tester.getCaveats()).upper(), 'KILLED BY PBS')

                # We _should_ actually have no results
                elif job_id == 1 and result_list != None:
                    self.fail('Failed: A job that should not have launched attempted to do so')

    def testPBSUnknownQstat(self):
        """
        Test QueueManager's ability to handle unknown qstat output
        """

        # Launch jobs correctly to create a session file (QUEUED status)
        self.testPBSGoodLaunch()

        with self.harnessDAG() as harness_dag:
            (harness, job_dag) = harness_dag

            for job_id, job in self.yieldJobs(job_dag):
                qstat_return = self.getQstatOutput(job_id=job_id, job_state='gibberish')
                result_list = self._mockSinglePBSLaunch(harness, job, qstat_return)

                # Join the results, because the TestHarness receives them out-of-order
                if job_id == 0:
                    join_results = ' '.join([result_list[0], result_list[1]])
                    self.assertRegexpMatches(join_results, 'FAILED \(UNKNOWN PBS STATUS\)')
                    self.assertRegexpMatches(join_results, 'SKIP')

                # We _should_ actually have no results
                elif job_id == 1 and result_list != None:
                    self.fail('Failed: A job that should not have launched attempted to do so')

    def testPBSProcessResults(self):
        """
        Test QueueManager's ability to call processResults when
        appropriate.

        Note: We can't actually 'processResults' and get a passing test
        because we didn't really run an application that generated a
        result. Instead we look for the error that happens when there is
        no output to test.
        """
        # Launch jobs to create a session file (QUEUED status)
        self.testPBSGoodLaunch()

        with self.harnessDAG() as harness_dag:
            (harness, job_dag) = harness_dag

            for job_id, job in self.yieldJobs(job_dag):
                qstat_return = self.getQstatOutput(job_id=job_id, job_state='F')
                result_list = self._mockSinglePBSLaunch(harness, job, qstat_return)

                # Join the results, because the TestHarness receives them out-of-order
                if job_id == 0:
                    join_results = ' '.join([result_list[0], result_list[1]])
                    self.assertRegexpMatches(join_results, 'FAILED \(NO STDOUT FILE\)')
                    self.assertRegexpMatches(join_results, 'SKIP')

                # We _should_ actually have no results
                elif job_id == 1 and result_list != None:
                    self.fail('Failed: A job that should not have launched attempted to do so')

    def testPBSGoodQstat(self):
        """
        Test QueueManager's ability to detect a PBS status using qstat
        """

        # Launch jobs to create a session file (QUEUED status)
        self.testPBSGoodLaunch()

        # Statuses to check for
        status_list = ['RUNNING', 'EXITING', 'QUEUED', 'HOLDING']
        for status in status_list:

            with self.harnessDAG() as harness_dag:
                (harness, job_dag) = harness_dag

                for job_id, job in self.yieldJobs(job_dag):
                    qstat_return = self.getQstatOutput(job_id=job_id, job_state=status[0])
                    result_list = self._mockSinglePBSLaunch(harness, job, qstat_return)
                    if len(result_list) == 1:
                        self.assertRegexpMatches(result_list[0], status)
                    else:
                        self.fail('Failed: QueueManager launched one job, but received several results')

            # Secondary checks; lets verify the session file is saving
            # the results properly
            if os.path.exists(self.pbs_session_file):
                with open(self.pbs_session_file, 'r') as session_file:
                    session = json.load(session_file)
            else:
                self.fail('Failed: Session File was not created')

            for key, value in session.iteritems():
                if key != 'QUEUEMANAGER':
                    self.assertRegexpMatches(session[key]['caveat_message'], status)

    def testSessionFile(self):
        """
        Verify the session file is what we expect
        """

        # Launch jobs to create a session file for us to use
        self.testPBSGoodLaunch()

        if os.path.exists(self.pbs_session_file):
            with open(self.pbs_session_file, 'r') as session_file:
                session = json.load(session_file)
        else:
            self.fail('Failed: Session File was not created')

        # Iterate over the session file and do some checks
        if 'QUEUEMANAGER' not in session.keys():
            self.fail('Failed: QUEUEMANAGER configuration key not found')

        checks = ['job_id',
                  'queue_script',
                  'job_name',
                  'working_dir',
                  'caveat_message',
                  'status_bucket']

        for key, value in session.iteritems():
            if key != 'QUEUEMANAGER':
                if set(checks) - set(value.keys()):
                    raise Exception('Failed: Invalid items present in session or this unittest is out of date!')

    def testCopyFiles(self):
        """
        Test the QueueManagers ability to copy specific files when requested.

        We will create two testers with a single dependency. The first tester,
        when launched, should not have copied any extra files, and that will
        be our first test.

        The second test does copy extra files.
        """

        with self.harnessDAG() as harness_dag:
            (harness, job_dag) = harness_dag

            for job_id, job in self.yieldJobs(job_dag):

                self._mockSinglePBSLaunch(harness, job, '%d.nowhere.org' % (job_id))

                # we should not have extra files copied after queueing the first job
                if job_id == 0 and os.path.exists(os.path.join(self.session_dir, 'queue_job_copy_file_test')):
                    self.fail('Failed: A file was copied that should not have been')

                # but our second job should have copied some extra stuff over
                elif job_id == 1 and not os.path.exists(os.path.join(self.session_dir, 'queue_job_copy_file_test')):
                    self.fail('Failed: A file was not copied that should have been')

    def testCleanup(self):
        """
        Test the QueueManagers ability to clean up after itself
        """

        # Launch jobs to create something we need to clean up
        self.testPBSGoodLaunch()

        # See if we actually made a mess, like we should have
        if os.path.exists(self.session_dir):
            harness = TestHarness(['foo', '--queue-cleanup', self.pbs_session_file], os.getenv('MOOSE_DIR'), app_name='moose')

            # Perform the clean up code
            harness.scheduler.cleanUp()
            if os.path.exists(os.path.join(self.session_dir)):
                self.fail('Failed to perform session cleanup')

        else:
            self.fail('Failed to create session: %s' % (self.session_dir))

if __name__ == '__main__':
    unittest.main(verbosity=2)
