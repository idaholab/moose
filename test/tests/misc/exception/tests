[Tests]
  # This example throws an exception during computeResidual() in the
  # first timestep, and then continues running with a reduced
  # timestep.  This example requires PETSc >= 3.6, otherwise you get:
  # PETSC ERROR: Petsc has generated inconsistent data
  # PETSC ERROR: Computed Nan differencing parameter h
  [./parallel_exception_residual_transient]
    type = 'Exodiff'
    input = 'parallel_exception_residual_transient.i'
    petsc_version = '>=3.6.0'
    exodiff = 'parallel_exception_residual_transient_out.e'
    use_old_floor = true
  [../]

  [./parallel_exception_residual_transient_non_zero_rank]
    type = 'Exodiff'
    input = 'parallel_exception_residual_transient.i'
    petsc_version = '>=3.6.0'
    cli_args = 'Kernels/exception/rank=1'
    exodiff = 'parallel_exception_residual_transient_out.e'
    use_old_floor = true
    prereq = 'parallel_exception_residual_transient'
    min_parallel = 2
    # The mesh is too small, and the number of elements on MPI rank 1 is 0 when we are uing 16 MPI processes. This happens with PETSc-3.8.3
    max_parallel = 10 
  [../]

  # This example throws an exception during computeJacobian() in the
  # first timestep, and then continues running with a reduced
  # timestep.
  [./parallel_exception_jacobian_transient]
    type = 'Exodiff'
    input = 'parallel_exception_jacobian_transient.i'
    petsc_version = '>=3.6.0'
    exodiff = 'parallel_exception_jacobian_transient_out.e'
    use_old_floor = true
  [../]

  [./parallel_exception_jacobian_transient_non_zero_rank]
    type = 'Exodiff'
    input = 'parallel_exception_jacobian_transient.i'
    petsc_version = '>=3.6.0'
    cli_args = 'Kernels/exception/rank=1'
    exodiff = 'parallel_exception_jacobian_transient_out.e'
    use_old_floor = true
    prereq = 'parallel_exception_jacobian_transient'
    min_parallel = 2
    # The mesh is too small, and the number of elements on MPI rank 1 is 0 when we are uing 16 MPI processes. This happens with PETSc-3.8.3
    max_parallel = 10
  [../]

  [./parallel_exception_initial_condition]
    type = RunApp
    input = 'parallel_exception_initial_condition.i'
    petsc_version = '>=3.5.0'
    expect_out = 'MooseException thrown during initial condition computation'
  [../]

  # Test that we can trigger an error on a non-zero rank and exit as expected
  [./parallel_error_residual_transient_non_zero_rank]
    type = 'RunException'
    input = 'parallel_exception_residual_transient.i'
    petsc_version = '>=3.6.0'
    min_parallel = 2
    cli_args = 'Kernels/exception/rank=1 Kernels/exception/should_throw=false Outputs/exodus=false'
    expect_err = 'Intentional error triggered during residual calculation'
    # The mesh is too small, and the number of elements on MPI rank 1 is 0 when we are uing 16 MPI processes. This happens with PETSc-3.8.3
    max_parallel = 10
  [../]

  [./parallel_error_jacobian_transient_non_zero_rank]
    type = 'RunException'
    input = 'parallel_exception_residual_transient.i'
    petsc_version = '>=3.6.0'
    min_parallel = 2
    cli_args = 'Kernels/exception/rank=1 Kernels/exception/should_throw=false Outputs/exodus=false'
    expect_err = 'Intentional error triggered during residual calculation'
    # The mesh is too small, and the number of elements on MPI rank 1 is 0 when we are uing 16 MPI processes. This happens with PETSc-3.8.3
    max_parallel = 10
  [../]
[]
